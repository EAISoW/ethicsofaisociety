<!DOCTYPE html>
<html>
<head>
    <title>Ethics of AI Society of Waterloo</title>

    <style>
        body {
            background-color: #232A44;
            color: white;
            font-family: Arial, sans-serif;
            text-align: left;
        }

        a {
            color: #A9C7FF;
        }

        table {
    width: 40%;
    border-collapse: collapse;
}

td, th {
    border: 1px solid #ccc;
    padding: 10px;
    vertical-align: top;
}

th {
    background-color: #000000;
    text-align: left;
}

.section-header {
    background-color: #cc0000;
    font-weight: bold;
    font-size: 1.1em;
}

.meta {
    background-color: #232A44;
}

ul {
    margin-top: 5px;
}
</style>
   
</head>

<body>
<center>

<a href="https://eaisow.github.io/ethicsofaisociety/">
    <img src="images/eais.png" width="700"></center>
</a>
    
    

    <p>
Previous Meetings
    </p>


</head>

<body>

<table>

<tr>
<h1>Ethics of AI Society — Briefing Notes</h1>

<div class="meta">
<p><strong>Date:</strong> February 13, 2026<br>
<strong>Time:</strong> 4:00–5:17 PM (approx.)<br>
<strong>Location:</strong> University of Waterloo<br>
<strong>Guest Speaker:</strong> David L. Hildebrand, Assistant Professor of Philosophy, University of Colorado Denver<br>
<strong>Chair:</strong> Matthew S. W. Silk</p>
</div>

<div class="section">
<h2>1. Opening Updates (Pre-Transcript)</h2>

<h3>Symposium – March 13</h3>
<ul>
<li>Planning for the March symposium is nearly complete.</li>
<li>Speaker slots are essentially full.</li>
<li>The event will conclude with a <strong>panel discussion</strong> (participants TBD).</li>
</ul>
</div>

<div class="section">
<h2>2. Report From David Hildebrand — International AI Conference in Paris</h2>

<p>David summarized insights from his recent talk in Paris on AI, privacy, and surveillance.</p>

<h3>Key Points from Hildebrand’s Report</h3>

<h4>EU Digital Wallet Initiative</h4>
<ul>
<li>EU is legally required to roll out government-authorized digital identity wallets by end of 2026.</li>
<li>Goal: allow citizens to share only the <strong>minimal</strong> information required for transactions.</li>
<li>Could improve privacy by reducing oversharing of personal data.</li>
</ul>

<h4>Privacy Requires Transparency</h4>
<ul>
<li>You can only know you <strong>have</strong> privacy if you know <strong>how and when</strong> you’re being watched.</li>
<li>Modern “hypernudging” and subtle surveillance undermine this awareness.</li>
</ul>

<h4>Surveillance Concerns</h4>
<ul>
<li>Hildebrand drew on Foucault’s concepts (panopticon, governmentality).</li>
<li>Citizens begin to self-monitor when uncertain about surveillance.</li>
<li>Palantir’s presence in Denver illustrates real-world risks of concentrated data power.</li>
</ul>
</div>

<div class="section">
<h2>3. Discussion: AI, Privacy, and Data Governance</h2>

<h3>3.1 Privacy as a Philosophical and Practical Problem</h3>
<ul>
<li>Privacy is <strong>complex</strong>, evolving with technology.</li>
<li>AI systems raise questions about:
    <ul>
        <li>What counts as private information?</li>
        <li>Whether inferences made without explicit data violate privacy.</li>
        <li>How prediction and personalization blur public/private boundaries.</li>
    </ul>
</li>
</ul>

<h3>3.2 Malicious Use, Malfunction, and Systemic Risks</h3>
<ul>
<li>Raised via the International AI Safety Report (2026).</li>
<li>AI systems can perform complex tasks but still fail at basic details (e.g., citations).</li>
<li>Digital wallets pose risks of:
    <ul>
        <li>Data breaches</li>
        <li>Authentication errors</li>
        <li>Misuse of central infrastructure</li>
    </ul>
</li>
</ul>

<h3>3.3 Human Behaviour: Over-Sharing</h3>
<ul>
<li>Users frequently give AI more data than necessary (e.g., resumes, personal context).</li>
<li>AI can reinforce confirmation bias by personalizing interactions.</li>
</ul>

<h3>3.4 Data Retention and Deletion</h3>
<ul>
<li>Hildebrand noted systems like Claude claim short-term memory and non-retention of user data.</li>
<li>But without public oversight, this relies on corporate trust.</li>
<li>A public, democratically accountable AI system could allow meaningful audits.</li>
</ul>

<h3>3.5 Privacy as Property?</h3>
<ul>
<li>Question raised: <strong>Should individuals legally own their data?</strong></li>
<li>Property framing could allow legal recourse if companies retain data beyond agreed terms.</li>
<li>Example: Matthew McConaughey trademarking his face to prevent deepfake misuse.</li>
</ul>

<h3>3.6 Digital Inclusion Concerns</h3>
<ul>
<li>Digital wallets assume citizens have smartphones.</li>
<li>Raises equity issues: should governments provide devices to ensure fair access?</li>
</ul>
</div>

<div class="section">
<h2>4. Privacy, Democracy, and Deweyan Theory</h2>

<h3>Privacy as a Democratic Good</h3>
<ul>
<li>Democracy relies on citizens engaging in inquiry free from coercive oversight.</li>
<li>Surveillance undermines:
    <ul>
        <li>Cognitive freedom</li>
        <li>Experimental thinking</li>
        <li>Equal footing in public discourse</li>
    </ul>
</li>
</ul>

<h3>Asymmetries of Power</h3>
<ul>
<li>AI systems accumulate unknown quantities of data.</li>
<li>Citizens cannot easily determine what is known about them.</li>
<li>This erodes democratic agency.</li>
</ul>
</div>

<div class="section">
<h2>5. Transition to Discussion on AI and Warfare</h2>

<h3>5.1 Overview of AI Use Cases in Warfare (Silk)</h3>
<ul>
<li><strong>Logistics:</strong> resource modelling, predictive maintenance</li>
<li><strong>Intelligence:</strong> satellite analysis, surveillance + facial recognition</li>
<li><strong>Cyber warfare:</strong> offense/defense, automated attacks</li>
<li><strong>Information warfare:</strong> propaganda, deepfakes, psychological operations</li>
<li><strong>Wargaming & simulation:</strong> modelling battlefield outcomes</li>
<li><strong>Autonomous weapons:</strong> drones, guided missiles</li>
<li><strong>Fire control systems:</strong> missile defense, high-speed interception</li>
</ul>

<h3>5.2 Psychological & Information Warfare</h3>
<ul>
<li>AI-powered propaganda may outweigh traditional weapons in impact.</li>
<li>Deepfakes can destabilize governments.</li>
<li>“Psyops” may become the primary battlefield.</li>
</ul>
</div>

<div class="section">
<h2>6. Ethical Issues in AI-Enabled Warfare</h2>

<h3>6.1 Proportionality & Discrimination</h3>
<ul>
<li>AI may misclassify targets (e.g., tank vs. hospital with similar shapes).</li>
<li>Raises classic Just War concerns:
    <ul>
        <li>Proportionality</li>
        <li>Civilian protection</li>
        <li>Combatant–noncombatant discrimination</li>
    </ul>
</li>
</ul>

<h3>6.2 Responsibility & Accountability Gaps</h3>
<ul>
<li>Who is responsible when AI makes a lethal mistake?</li>
<li>AI diffuses responsibility and removes clear decision chains.</li>
<li>Danger of leaders hiding behind AI recommendations (“veneer of accountability”).</li>
</ul>

<h3>6.3 Algorithmic Opacity Enhances Existing Problems</h3>
<ul>
<li>Similar to prior corporate fragmentation of responsibility, but amplified.</li>
<li>Bias in training data directly affects battlefield decisions.</li>
</ul>

<h3>6.4 Lower Human Cost = Easier to Justify War</h3>
<ul>
<li>If no soldiers die, domestic resistance to war diminishes.</li>
<li>Risk of normalizing conflict as a <strong>default</strong> problem-solving tool.</li>
</ul>
</div>

<div class="section">
<h2>7. Arms Races, Geopolitics, and the Prisoner’s Dilemma</h2>
<ul>
<li>If one nation builds AI weapons, others feel compelled to follow.</li>
<li>Similar to nuclear arms race dynamics.</li>
<li>China’s technical leap (e.g., IPv6 infrastructure) discussed as a real-world example.</li>
</ul>
</div>

<div class="section">
<h2>8. Collaboration Between Humans and AI</h2>
<ul>
<li>Not necessarily AI replacing humans; rather <strong>hybrid systems</strong> where humans remain involved.</li>
<li>Example: University of Waterloo admissions use joint human + AI evaluation.</li>
<li>Multiple independent AI systems plus human oversight may increase reliability.</li>
</ul>
</div>

<div class="section">
<h2>9. Organizational Matters</h2>
<ul>
<li>Environmental impacts of AI postponed to next meeting.</li>
<li>Future topics:
    <ul>
        <li>Transparent AI</li>
        <li>Trustworthy AI</li>
        <li>Environmental cost of AI systems</li>
    </ul>
</li>
</ul>

<p><strong>Next Meeting:</strong> February 27, 4:00 PM</p>
</tr>

</table>

</body>
</html>
