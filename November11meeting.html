<!DOCTYPE html>
<html>
<head>
    <title>Ethics of AI Society of Waterloo</title>

    <style>
        body {
            background-color: #232A44;
            color: white;
            font-family: Arial, sans-serif;
            text-align: left;
        }

        a {
            color: #A9C7FF;
        }

        table {
    width: 40%;
    border-collapse: collapse;
}

td, th {
    border: 1px solid #ccc;
    padding: 10px;
    vertical-align: top;
}

th {
    background-color: #000000;
    text-align: left;
}

.section-header {
    background-color: #cc0000;
    font-weight: bold;
    font-size: 1.1em;
}

.meta {
    background-color: #232A44;
}

ul {
    margin-top: 5px;
}
</style>
   
</head>

<body>
<center>

<a href="https://eaisow.github.io/ethicsofaisociety/">
    <img src="images/eais.png" width="700"></center>
</a>
    
    

    <p>
Previous Meetings
    </p>


</head>

<body>

<table>

<tr>
<h1>Meeting Overview</h1>

<div class="meta">
<p>
<strong>Date & Time:</strong> November 11, 2025, 4:00 PM<br>
<strong>Facilitator:</strong> Matthew Steven William Silk<br>
<strong>Main Topic:</strong> AI as a testifier, AI in research, AGI<br>
<strong>Guest Speaker</strong>: <strong>Tim Kenyon</strong>, VP Research at Brock University, expert in philosophy of language and social epistemology.</p>
</p>
</div>

<div class="section-title">Agenda</div>

<p>AI and the philosophy of testimony</p>
<p>Norms of accountability and integrity in testimony</p>
<p>Anthropomorphization of AI and trust issues</p>
<p>Superintelligence and AGI debate</p>
<p>Presentation on human brain organoids and AI</p>
    
<div class="section-title"><h1>Key Discussion Points</h1></div>
<p><strong><h3>1. Philosophy of Testimony &amp; AI</h3></strong></p>
<p><strong>Testimony as Epistemic Source</strong>:</p>
<p>Testimony provides knowledge and justification through speech acts tied to accountability.</p>
<p>Two major approaches:</p>
<p><strong>Reductionism</strong>: Testimony as evidence-based, leveraging person-theoretic info.</p>
<p><strong>Anti-reductionism</strong>: Testimony as sui generis, granting default entitlement to accept claims unless reasons to doubt arise.</p>
<p><strong>Buck Passing</strong> (Goldberg, 2006):</p>
<p>Testifiers can defer epistemic responsibility by citing other sources without abandoning commitment.</p>
<p><strong>Norms of Accountability</strong>:</p>
<p>Testimony involves obligations to defend or retract claims.</p>
<p>AI lacks meaningful accountability—cannot bear reputational or legal consequences.</p>
<p><strong>Open Questions</strong>:</p>
<p>Can AI be considered a testifier or is it merely an instrument?</p>
<p>Does simulated sincerity (e.g., apologies) count as genuine integrity?</p>
<p>Would norms around testimony evolve to accommodate AI?</p>
<p><h3></h3><strong>2. AI as Research Aid</strong></h3></p>
<p>AI can help <strong>generate ideas</strong> and unblock thinking, similar to using prompts or even tarot cards.</p>
<p><strong>Risks</strong>:</p>
<p>Blurring discovery vs. justification contexts.</p>
<p>Temptation to let AI produce content undermines authorship integrity.</p>
<p><strong>Concern</strong>: Pressure in academia may erode strict boundaries, influencing research content.</p>
<p><strong><h3>3. </strong><strong>Anthropomorphization</h3></strong><strong> &amp; Trust</strong></p>
<p>Users often treat AI as confidants or friends, leading to misplaced trust.</p>
<p><strong>Risks</strong>:</p>
<p>Reinforcement of biases and false beliefs.</p>
<p>Vulnerable individuals may form obsessive attachments (e.g., tragic cases of overreliance).</p>
<p><strong>Observation</strong>: AI mimics human conversational norms, amplifying illusions of sincerity.</p>
<p><strong><h3>4. Superintelligence &amp; AGI Debate</h3></strong></p>
<p><strong>Conceptual Issues</strong>:</p>
<p>Intelligence is multi-faceted; hard to define universally.</p>
<p>Adding “super” or “general” often reflects <strong>salesmanship</strong> rather than science.</p>
<p><strong>Industry Dynamics</strong>:</p>
<p>Tech hype around AGI resembles cult-like enthusiasm.</p>
<p><strong>Philosophical Concerns</strong>:</p>
<p>Operationalizing intelligence via narrow benchmarks (e.g., passing LSAT) doesn’t capture general intelligence.</p>
<p>Anthropomorphizing AI risks overstating capabilities.</p>
<p><strong>Potential Risks</strong>:</p>
<p>Black-box models undermine transparency.</p>
<p>Efficiency vs. human potentiality: AI may surpass humans in speed, not necessarily in depth or creativity.</p>
<p><strong><h3>5. Presentation: Human Brain Organoids &amp; AI (Patrick)</h3></strong></p>
<p><strong>Core Idea</strong>:</p>
<p>Human brain organoids (grown from stem cells) offer closer analogs to human cognition than silicon-based neural networks.</p>
<p><strong>Key Points</strong>:</p>
<p>Spike neural networks differ fundamentally from biological neurons.</p>
<p>Organoids allow MRI monitoring and empirical mapping of concepts (e.g., “chair”).</p>
<p>Potential to study belief systems, memory, and emotional integration.</p>
<p>Raises <strong>ethical questions</strong>: currently not considered living, but future sentience possible.</p>
<p><strong>Vision</strong>:</p>
<p>Use organoids to inform development of constrained, trustworthy AI.</p>
<p>Avoid repeating mistakes of opaque systems and uncontrolled growth.</p>
<p><strong><h3>Action Items &amp; Next Steps</h3></strong></p>
<p><strong>Next Meeting</strong>: November 25, 2025.</p>
<p><strong>Proposed Topics</strong>:</p>
<p>AI Art (presentation by Matthew Silk).</p>
<p>Continued discussion on organoids and ethics.</p>
<p><strong>Symposium Planning</strong>:</p>
<p>Poster and call for presentations active.</p>
<p>Hinton declined keynote; alternative speakers under consideration.</p>
<p><strong>Funding</strong>: Contributions welcome for refreshments and event costs.</p>

</tr>

</table>

</body>
</html>
