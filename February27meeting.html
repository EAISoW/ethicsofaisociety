<!DOCTYPE html>
<html>
<head>
    <title>Ethics of AI Society of Waterloo</title>

    <style>
        body {
            background-color: #232A44;
            color: white;
            font-family: Arial, sans-serif;
            text-align: left;
        }

        a {
            color: #A9C7FF;
        }

        table {
    width: 40%;
    border-collapse: collapse;
}

td, th {
    border: 1px solid #ccc;
    padding: 10px;
    vertical-align: top;
}

th {
    background-color: #000000;
    text-align: left;
}

.section-header {
    background-color: #cc0000;
    font-weight: bold;
    font-size: 1.1em;
}
.subsection {
    margin-top: 1em;
}
.meta {
    background-color: #232A44;
}
.indent {
    margin-left: 2em;
}
ul {
    margin-top: 5px;
}
</style>
   
</head>

<body>
<center>

<a href="https://eaisow.github.io/ethicsofaisociety/">
    <img src="images/eais.png" width="700"></center>
</a>
    
    

    <p>
Previous Meetings
    </p>


</head>

<body>

<table>

<tr>
<h1>Briefing Notes – Ethics of AI Society Meeting</h1>

<p><strong>Date:</strong> February 27, 2026<br>
<strong>Time:</strong> 4:00 PM–5:20 PM<br>
<strong>Location:</strong> University of Waterloo<br>
<strong>Chair:</strong> Matthew S.W. Silk</p>

<div class="section">
<h2>1. Administrative Updates</h2>

<div class="subsection">
<h3>1.1 New Website Launch</h3>
<p>The group now has an official website hosting:</p>
<ul>
<li>Meeting archives</li>
<li>Media contributions</li>
<li>Space for potential short articles on AI ethics</li>
</ul>
<p>Possibility discussed of <em>future newsletters or publications</em> if volunteers join for editing and writing.</p>
</div>

<div class="subsection">
<h3>1.2 Meeting Milestone</h3>
<p>This session marks the <strong>20th official meeting</strong> of the Ethics of AI Society.</p>
</div>

<div class="subsection">
<h3>1.3 Introductions</h3>
<p>Several new attendees introduced themselves, including graduate students, faculty, engineers, and returning members.</p>

<p>Topics of interest among attendees included:</p>
<ul>
<li>Medical AI</li>
<li>Pragmatism and philosophy of language</li>
<li>Digital communications</li>
<li>Racial profiling and AI</li>
<li>System design and engineering</li>
<li>Logic frameworks and Peircean graphs</li>
</ul>
</div>
</div>

<div class="section">
<h2>2. Symposium Planning (March 13th at Wilfrid Laurier)</h2>

<div class="subsection">
<h3>2.1 Logistics &amp; Needs</h3>
<p>Potential need for:</p>
<ul>
<li>A <strong>door volunteer</strong> to manage refreshments if they must remain outside the room.</li>
<li>A <strong>tech volunteer</strong> to assist with microphones and setup.</li>
<li>An <strong>additional microphone</strong> for audience audio capture during Q&amp;A.</li>
</ul>
</div>

<div class="subsection">
<h3>2.2 Keynote Panel</h3>
<p>Will include:</p>
<ul>
<li>Ian McDonald</li>
<li>Patricia Marino (Philosophy)</li>
<li>Ontario MPP Rob Cerjanec</li>
</ul>

<p>Members are invited to submit <strong>potential questions</strong> for the panel via a dedicated Teams chat.</p>

<p>Questions may relate to <em>ethics, policy, or public impacts of AI.</em></p>
</div>
</div>

<div class="section">
<h2>3. Topic Discussion #1 – Environmental Impacts of AI</h2>

<div class="subsection">
<h3>3.1 Opening Framing</h3>
<p>AI systems require:</p>
<ul>
<li>Large electrical power for training and inference</li>
<li>Significant water usage for datacenter cooling</li>
<li>Rare minerals for hardware manufacturing</li>
</ul>

<p><strong>Key overarching question:</strong> Is the environmental cost of AI “worth it”?</p>
</div>

<div class="subsection">
<h3>3.2 Main Themes Raised</h3>

<h4>A. Urgency of Climate Crisis</h4>
<p>Some participants argued that accelerating AI development worsens environmental degradation during an already critical climate period.</p>
<p>Concerns about society’s ability to adapt regulations and policies quickly enough.</p>

<h4>B. Individual vs Corporate Responsibility</h4>
<p>Debate on whether responsibility should lie more with:</p>
<ul>
<li>Tech companies (for creating energy-intensive systems)</li>
<li>Users (for unnecessary usage)</li>
</ul>
<p>Several members argued that blaming individuals is oversimplified; structural and corporate choices have far more impact.</p>

<h4>C. Question of “Necessary” vs “Excessive” AI Use</h4>
<p>No clear societal standards exist for what qualifies as necessary usage.</p>
<p>Examples:</p>
<ul>
<li>Medical decision support may justify resource expenditure.</li>
<li>Homework help and chatbots may not.</li>
</ul>

<h4>D. Efficiency Improvements Over Time</h4>
<p>Anticipated advancements:</p>
<ul>
<li>More efficient training</li>
<li>Better cooling methods</li>
<li>Smaller, more efficient chip architectures</li>
</ul>
<p>But: Greater efficiency may lead to <em>more total usage</em>, offsetting gains (rebound effect).</p>

<h4>E. Geographic and Ethical Implications</h4>
<p>Data centers often placed in:</p>
<ul>
<li>Areas with scarce water</li>
<li>Developing countries with lax environmental regulations</li>
</ul>
<p>Raises justice concerns about disproportionate harms.</p>

<h4>F. Competition and Innovation Pressures</h4>
<p>Limiting the number of AI models / supercomputers could reduce short-term environmental cost.</p>
<p>But competition drives companies to develop more efficient systems over time.</p>

<h4>G. Possible Alternative Computing Paradigms</h4>
<p>Mention of human brain organoid computing as a potential ultra-low-power alternative (25 watts), though extremely speculative and ethically complex.</p>

</div>
</div>

<div class="section">
<h2>4. Topic Discussion #2 – Trustworthy AI &amp; Duty to Report Threats</h2>

<div class="subsection">
<h3>4.1 Case Prompting Discussion</h3>
<p><strong>Recent news:</strong><br>
BC government criticized OpenAI for not notifying authorities after concerning user conversations from an individual later involved in the Tumbler Ridge shooting.</p>

<p><strong>Raises core question:</strong><br>
When should AI developers report harmful content to authorities?</p>
</div>

<div class="subsection">
<h3>4.2 Concerns &amp; Tradeoffs</h3>

<h4>A. Privacy vs Public Safety</h4>
<p><strong>Reporting too frequently risks:</strong></p>
<ul>
<li>Mass privacy violations</li>
<li>Wasted police resources</li>
<li>Chilling effects on legitimate use</li>
</ul>

<p><strong>Reporting too rarely risks:</strong></p>
<ul>
<li>Failure to prevent violence</li>
<li>Public distrust of AI companies</li>
</ul>

<h4>B. Difficulty Setting a “Reporting Threshold”</h4>
<p>Who decides when content is dangerous?</p>
<p>AI-detected red flags often generate false positives.</p>
<p>Even humans reviewing flagged data may disagree on risk severity.</p>

<h4>C. The Problem of Evasion</h4>
<p>Users can circumvent safeguards:</p>
<ul>
<li>Rephrasing questions</li>
<li>Asking in code (e.g., Morse)</li>
<li>Using reverse-engineering prompts</li>
<li>Running offline open-source models</li>
</ul>
<p>Therefore, even perfect reporting policies cannot catch all cases.</p>

<h4>D. Mental Health–Related Prompts</h4>
<p>AI may be used as a “sanctuary” for distressed users.</p>
<p>Overreporting suicide-related prompts could:</p>
<ul>
<li>Flag far too many cases</li>
<li>Reduce public trust</li>
<li>Push vulnerable people away from seeking help</li>
</ul>

<h4>E. Anthropomorphizing AI</h4>
<p>Chatbots often appear friendly or supportive.</p>
<p>Members raised concerns that:</p>
<ul>
<li>Politeness consumes resources</li>
<li>It reinforces emotional dependence</li>
<li>It encourages confirmation bias</li>
</ul>

<p><strong>Suggestions:</strong></p>
<ul>
<li>Stronger disclaimers reminding users the AI is not human</li>
<li>More neutral tone to reduce emotional bonding</li>
<li>Reduced sycophantic behavior</li>
</ul>

<h4>F. Government vs Corporate Pressure</h4>
<p>Canadian and U.S. governments pushing in opposite directions:</p>
<ul>
<li>Canada wants more safety &amp; reporting.</li>
<li>U.S. agencies want fewer restrictions (e.g., Pentagon pressuring Anthropic; Grok being used in secure networks).</li>
</ul>

</div>
</div>

<div class="section">
<h2>5. Future Meeting Plans</h2>

<h3>Next Meeting Date</h3>
<p>March 20, after the symposium.</p>

<h3>Tentative Topics</h3>
<ul>
<li>Symposium debrief</li>
<li>Issues in:
    <ul>
        <li>Model validation</li>
        <li>Confirmation bias in AI</li>
        <li>Opacity and black-box systems</li>
    </ul>
</li>
</ul>

</div>



</tr>

</table>

</body>
</html>
